{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "yMQW1u2h6UPH",
        "outputId": "b9f89158-d917-463b-ec11-09ad0e13f20e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9b8487a2-2473-4772-9742-af0b2c55e254\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9b8487a2-2473-4772-9742-af0b2c55e254\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving box.zip to box.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVT-vSNO6Za0",
        "outputId": "1a8d8ada-29ec-4ab5-c046-f0a24de8b258"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_RBVkUw6E-I",
        "outputId": "f4d49d18-a782-4d0e-f04d-35898a66c98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Sp2GNO for PCA-compressed Time Series Prediction\n",
            "Exact Architecture from Paper - FIXED VERSION\n",
            "======================================================================\n",
            "PyTorch version: 2.8.0+cu126\n",
            "Device: cpu\n",
            "Random seed: 42\n",
            "======================================================================\n",
            "Extracting /content/box.zip to /tmp/tmp1wplm40n...\n",
            "Extraction complete. Data root: /tmp/tmp1wplm40n/Dataset for box girder\n",
            "\n",
            "Found 12 cases\n",
            "Training cases: 8\n",
            "Test cases: 4\n",
            "\n",
            "======================================================================\n",
            "STEP 1: Fitting PCA on training data\n",
            "======================================================================\n",
            "Training label matrix shape: (624, 8231)\n",
            "Explained variance (first 5): [0.8087 0.0963 0.0287 0.0188 0.0164]\n",
            "Cumulative explained variance: 1.0000\n",
            "\n",
            "======================================================================\n",
            "STEP 2: Creating graph datasets with Sp2GNO preprocessing\n",
            "======================================================================\n",
            "Train dataset size: 8\n",
            "Test dataset size: 4\n",
            "\n",
            "Input features: 5\n",
            "Number of nodes (sample): 70\n",
            "Output PCA components: 20\n",
            "Max Lipschitz embedding dimension: 20\n",
            "Edge feature dimension: 16\n",
            "Number of eigenvectors (m): 16\n",
            "\n",
            "======================================================================\n",
            "STEP 3: Initializing Sp2GNO model (Paper Architecture)\n",
            "======================================================================\n",
            "Model parameters: 264,744\n",
            "Number of Sp2GNO blocks: 3\n",
            "Hidden dimension: 64\n",
            "Device: cpu\n",
            "\n",
            "======================================================================\n",
            "STEP 4: Training Sp2GNO\n",
            "======================================================================\n",
            "Epoch 1/3000 - Loss: 5.364321 - Best: 5.364321 - LR: 5.00e-04\n",
            "✓ New best model saved at epoch 10 with loss 2.459950\n",
            "✓ New best model saved at epoch 20 with loss 0.971763\n",
            "✓ New best model saved at epoch 30 with loss 0.319332\n",
            "✓ New best model saved at epoch 50 with loss 0.133431\n",
            "Epoch 50/3000 - Loss: 0.133431 - Best: 0.133431 - LR: 5.00e-04\n",
            "✓ New best model saved at epoch 60 with loss 0.102422\n",
            "✓ New best model saved at epoch 80 with loss 0.015190\n",
            "✓ New best model saved at epoch 100 with loss -0.019538\n",
            "Epoch 100/3000 - Loss: -0.019538 - Best: -0.019538 - LR: 5.00e-04\n",
            "✓ New best model saved at epoch 150 with loss -0.055815\n",
            "Epoch 150/3000 - Loss: -0.055815 - Best: -0.055815 - LR: 5.00e-04\n",
            "Epoch 200/3000 - Loss: -0.074765 - Best: -0.074863 - LR: 5.00e-04\n",
            "✓ New best model saved at epoch 210 with loss -0.080417\n",
            "✓ New best model saved at epoch 220 with loss -0.082668\n",
            "✓ New best model saved at epoch 250 with loss -0.094909\n",
            "Epoch 250/3000 - Loss: -0.094909 - Best: -0.094909 - LR: 5.00e-04\n",
            "Epoch 300/3000 - Loss: -0.099755 - Best: -0.102890 - LR: 5.00e-04\n",
            "✓ New best model saved at epoch 310 with loss -0.107366\n",
            "Epoch 350/3000 - Loss: -0.110207 - Best: -0.112491 - LR: 5.00e-04\n",
            "✓ New best model saved at epoch 380 with loss -0.119955\n",
            "Epoch 400/3000 - Loss: -0.107069 - Best: -0.123121 - LR: 5.00e-04\n",
            "✓ New best model saved at epoch 430 with loss -0.137231\n",
            "Epoch 450/3000 - Loss: -0.130170 - Best: -0.142242 - LR: 2.50e-04\n",
            "✓ New best model saved at epoch 470 with loss -0.143490\n",
            "Epoch 500/3000 - Loss: -0.140531 - Best: -0.145794 - LR: 2.50e-04\n",
            "Epoch 550/3000 - Loss: -0.148251 - Best: -0.152085 - LR: 2.50e-04\n",
            "Epoch 600/3000 - Loss: -0.161294 - Best: -0.162838 - LR: 1.25e-04\n",
            "Epoch 650/3000 - Loss: -0.165942 - Best: -0.166261 - LR: 1.25e-04\n",
            "Epoch 700/3000 - Loss: -0.171813 - Best: -0.172470 - LR: 6.25e-05\n",
            "Epoch 750/3000 - Loss: -0.173032 - Best: -0.174536 - LR: 6.25e-05\n",
            "✓ New best model saved at epoch 790 with loss -0.176643\n",
            "Epoch 800/3000 - Loss: -0.176793 - Best: -0.177200 - LR: 6.25e-05\n",
            "✓ New best model saved at epoch 830 with loss -0.178730\n",
            "✓ New best model saved at epoch 840 with loss -0.180430\n",
            "Epoch 850/3000 - Loss: -0.181006 - Best: -0.181060 - LR: 3.13e-05\n",
            "Epoch 900/3000 - Loss: -0.181819 - Best: -0.182633 - LR: 3.13e-05\n",
            "✓ New best model saved at epoch 910 with loss -0.182966\n",
            "✓ New best model saved at epoch 950 with loss -0.184620\n",
            "Epoch 950/3000 - Loss: -0.184620 - Best: -0.184620 - LR: 3.13e-05\n",
            "✓ New best model saved at epoch 990 with loss -0.186115\n",
            "✓ New best model saved at epoch 1000 with loss -0.186404\n",
            "Epoch 1000/3000 - Loss: -0.186404 - Best: -0.186404 - LR: 1.56e-05\n",
            "✓ New best model saved at epoch 1030 with loss -0.187073\n",
            "Epoch 1050/3000 - Loss: -0.187420 - Best: -0.187476 - LR: 1.56e-05\n",
            "✓ New best model saved at epoch 1070 with loss -0.187726\n",
            "Epoch 1100/3000 - Loss: -0.188229 - Best: -0.188360 - LR: 1.56e-05\n",
            "✓ New best model saved at epoch 1110 with loss -0.188707\n",
            "Epoch 1150/3000 - Loss: -0.188698 - Best: -0.189336 - LR: 1.56e-05\n",
            "Epoch 1200/3000 - Loss: -0.190238 - Best: -0.190363 - LR: 1.56e-05\n",
            "✓ New best model saved at epoch 1210 with loss -0.190458\n",
            "✓ New best model saved at epoch 1230 with loss -0.190739\n",
            "Epoch 1250/3000 - Loss: -0.190154 - Best: -0.191110 - LR: 1.56e-05\n",
            "✓ New best model saved at epoch 1300 with loss -0.191995\n",
            "Epoch 1300/3000 - Loss: -0.191995 - Best: -0.191995 - LR: 1.56e-05\n",
            "✓ New best model saved at epoch 1320 with loss -0.192351\n",
            "Epoch 1350/3000 - Loss: -0.192293 - Best: -0.192647 - LR: 1.56e-05\n",
            "✓ New best model saved at epoch 1370 with loss -0.193025\n",
            "✓ New best model saved at epoch 1400 with loss -0.193882\n",
            "Epoch 1400/3000 - Loss: -0.193882 - Best: -0.193882 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1410 with loss -0.194092\n",
            "✓ New best model saved at epoch 1440 with loss -0.194493\n",
            "Epoch 1450/3000 - Loss: -0.194524 - Best: -0.194549 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1490 with loss -0.195074\n",
            "Epoch 1500/3000 - Loss: -0.194907 - Best: -0.195144 - LR: 7.81e-06\n",
            "Epoch 1550/3000 - Loss: -0.195295 - Best: -0.195647 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1560 with loss -0.195747\n",
            "✓ New best model saved at epoch 1570 with loss -0.195840\n",
            "Epoch 1600/3000 - Loss: -0.196038 - Best: -0.196293 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1630 with loss -0.196586\n",
            "Epoch 1650/3000 - Loss: -0.196283 - Best: -0.196613 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1670 with loss -0.196955\n",
            "✓ New best model saved at epoch 1690 with loss -0.197206\n",
            "✓ New best model saved at epoch 1700 with loss -0.197225\n",
            "Epoch 1700/3000 - Loss: -0.197225 - Best: -0.197225 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1740 with loss -0.197642\n",
            "Epoch 1750/3000 - Loss: -0.197216 - Best: -0.197758 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1770 with loss -0.197802\n",
            "Epoch 1800/3000 - Loss: -0.198005 - Best: -0.198153 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1810 with loss -0.198314\n",
            "✓ New best model saved at epoch 1840 with loss -0.198561\n",
            "Epoch 1850/3000 - Loss: -0.198294 - Best: -0.198580 - LR: 7.81e-06\n",
            "Epoch 1900/3000 - Loss: -0.198999 - Best: -0.199043 - LR: 7.81e-06\n",
            "Epoch 1950/3000 - Loss: -0.199386 - Best: -0.199458 - LR: 7.81e-06\n",
            "✓ New best model saved at epoch 1980 with loss -0.199916\n",
            "✓ New best model saved at epoch 1990 with loss -0.199943\n",
            "✓ New best model saved at epoch 2000 with loss -0.200013\n",
            "Epoch 2000/3000 - Loss: -0.200013 - Best: -0.200013 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2010 with loss -0.200142\n",
            "✓ New best model saved at epoch 2050 with loss -0.200467\n",
            "Epoch 2050/3000 - Loss: -0.200467 - Best: -0.200467 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2080 with loss -0.200652\n",
            "✓ New best model saved at epoch 2100 with loss -0.200793\n",
            "Epoch 2100/3000 - Loss: -0.200793 - Best: -0.200793 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2110 with loss -0.200882\n",
            "✓ New best model saved at epoch 2150 with loss -0.201114\n",
            "Epoch 2150/3000 - Loss: -0.201114 - Best: -0.201114 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2170 with loss -0.201229\n",
            "Epoch 2200/3000 - Loss: -0.201351 - Best: -0.201392 - LR: 3.91e-06\n",
            "Epoch 2250/3000 - Loss: -0.201595 - Best: -0.201676 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2260 with loss -0.201804\n",
            "Epoch 2300/3000 - Loss: -0.201867 - Best: -0.201990 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2340 with loss -0.202246\n",
            "Epoch 2350/3000 - Loss: -0.202238 - Best: -0.202263 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2380 with loss -0.202471\n",
            "Epoch 2400/3000 - Loss: -0.202425 - Best: -0.202578 - LR: 3.91e-06\n",
            "Epoch 2450/3000 - Loss: -0.202632 - Best: -0.202828 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2460 with loss -0.202881\n",
            "✓ New best model saved at epoch 2480 with loss -0.203054\n",
            "✓ New best model saved at epoch 2490 with loss -0.203071\n",
            "Epoch 2500/3000 - Loss: -0.202997 - Best: -0.203138 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2530 with loss -0.203332\n",
            "Epoch 2550/3000 - Loss: -0.203223 - Best: -0.203368 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2580 with loss -0.203518\n",
            "Epoch 2600/3000 - Loss: -0.203507 - Best: -0.203593 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2610 with loss -0.203671\n",
            "✓ New best model saved at epoch 2620 with loss -0.203711\n",
            "✓ New best model saved at epoch 2630 with loss -0.203825\n",
            "Epoch 2650/3000 - Loss: -0.203267 - Best: -0.203891 - LR: 3.91e-06\n",
            "Epoch 2700/3000 - Loss: -0.203858 - Best: -0.204111 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2730 with loss -0.204341\n",
            "✓ New best model saved at epoch 2740 with loss -0.204373\n",
            "Epoch 2750/3000 - Loss: -0.204291 - Best: -0.204439 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2780 with loss -0.204600\n",
            "Epoch 2800/3000 - Loss: -0.204560 - Best: -0.204653 - LR: 3.91e-06\n",
            "✓ New best model saved at epoch 2820 with loss -0.204886\n",
            "✓ New best model saved at epoch 2830 with loss -0.204960\n",
            "✓ New best model saved at epoch 2850 with loss -0.205045\n",
            "Epoch 2850/3000 - Loss: -0.205045 - Best: -0.205045 - LR: 1.95e-06\n",
            "✓ New best model saved at epoch 2890 with loss -0.205201\n",
            "Epoch 2900/3000 - Loss: -0.205195 - Best: -0.205245 - LR: 1.95e-06\n",
            "✓ New best model saved at epoch 2910 with loss -0.205291\n",
            "✓ New best model saved at epoch 2930 with loss -0.205329\n",
            "Epoch 2950/3000 - Loss: -0.205407 - Best: -0.205447 - LR: 1.95e-06\n",
            "Epoch 3000/3000 - Loss: -0.205527 - Best: -0.205581 - LR: 1.95e-06\n",
            "\n",
            "Loaded best model from epoch 2983\n",
            "\n",
            "======================================================================\n",
            "STEP 5: Evaluation on test set\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2920663278.py:790: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  true_impulses = np.array([np.trapz(ts) for ts in true_ts])\n",
            "/tmp/ipython-input-2920663278.py:791: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  pred_impulses = np.array([np.trapz(ts) for ts in recon])\n",
            "/tmp/ipython-input-2920663278.py:790: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  true_impulses = np.array([np.trapz(ts) for ts in true_ts])\n",
            "/tmp/ipython-input-2920663278.py:791: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  pred_impulses = np.array([np.trapz(ts) for ts in recon])\n",
            "/tmp/ipython-input-2920663278.py:790: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  true_impulses = np.array([np.trapz(ts) for ts in true_ts])\n",
            "/tmp/ipython-input-2920663278.py:791: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  pred_impulses = np.array([np.trapz(ts) for ts in recon])\n",
            "/tmp/ipython-input-2920663278.py:790: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  true_impulses = np.array([np.trapz(ts) for ts in true_ts])\n",
            "/tmp/ipython-input-2920663278.py:791: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
            "  pred_impulses = np.array([np.trapz(ts) for ts in recon])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "----------------------------------------------------------------------\n",
            "26.6+1.9:\n",
            "  R²: 0.7799\n",
            "  RMSE: 0.0318\n",
            "  Peak MAPE: 0.1267\n",
            "  Impulse MAPE: 0.0892\n",
            "  Avg Variance: 0.1006\n",
            "\n",
            "20.6+1.9:\n",
            "  R²: 0.8224\n",
            "  RMSE: 0.0336\n",
            "  Peak MAPE: 0.1547\n",
            "  Impulse MAPE: 0.1258\n",
            "  Avg Variance: 0.1786\n",
            "\n",
            "20.6+10:\n",
            "  R²: 0.7903\n",
            "  RMSE: 0.0272\n",
            "  Peak MAPE: 0.1357\n",
            "  Impulse MAPE: 0.7684\n",
            "  Avg Variance: 0.0657\n",
            "\n",
            "31.3+15:\n",
            "  R²: 0.7323\n",
            "  RMSE: 0.0226\n",
            "  Peak MAPE: 0.1944\n",
            "  Impulse MAPE: 6.1567\n",
            "  Avg Variance: 0.0448\n",
            "\n",
            "======================================================================\n",
            "OVERALL TEST PERFORMANCE\n",
            "======================================================================\n",
            "Average R²: 0.7812\n",
            "Average RMSE: 0.0288\n",
            "Average Peak MAPE: 0.1529\n",
            "Average Impulse MAPE: 1.7850\n",
            "\n",
            "Model saved to: sp2gno_model.pt\n",
            "PCA saved to: sp2gno_pca.pkl\n",
            "\n",
            "======================================================================\n",
            "STEP 6: Generating visualizations\n",
            "======================================================================\n",
            "Visualizing case: 26.6+1.9\n",
            "Visualization saved to: sp2gno_results.png\n",
            "Overall performance plot saved to: sp2gno_overall_performance.png\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "GraphFNO for PCA-compressed Time Series Prediction on Graphs\n",
        "Implements the exact Sp2GNO architecture from the paper:\n",
        "\"Spatio-Spectral Graph Neural Operator for Solving Computational Mechanics Problems\"\n",
        "\n",
        "FIXED: Variable Lipschitz embedding dimension issue\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.utils import get_laplacian, to_dense_adj\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import scipy.sparse as sp\n",
        "from scipy.sparse.linalg import eigsh\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETERS (ANTI-OVERFITTING CONFIGURATION)\n",
        "# ============================================================================\n",
        "ZIP_FILE_PATH = \"/content/box.zip\"\n",
        "PCA_COMPONENTS = 20\n",
        "WIDTH = 64  # Reduced from 128 to prevent overfitting\n",
        "NUM_SP2GNO_BLOCKS = 3  # Reduced from 4 to simplify model\n",
        "K_CHEBYSHEV = 6  # Chebyshev polynomial order\n",
        "M_EIGENVECTORS = 16  # Reduced from 32 to prevent overfitting\n",
        "K_NEIGHBORS = 20  # K for KNN graph construction\n",
        "BATCH_SIZE = 2\n",
        "LR = 5e-4  # Reduced learning rate for better generalization\n",
        "EPOCHS = 3000\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SEED = 42\n",
        "\n",
        "# Regularization parameters\n",
        "DROPOUT_RATE = 0.15  # Dropout for regularization\n",
        "WEIGHT_DECAY = 1e-3  # L2 regularization (increased)\n",
        "GRAD_CLIP = 0.5  # Gradient clipping (reduced for stability)\n",
        "SMOOTHNESS_WEIGHT = 0.05  # Increased spatial smoothness regularization\n",
        "\n",
        "def set_seed(seed=SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed()\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "def extract_zip(zip_path, extract_to=None):\n",
        "    if extract_to is None:\n",
        "        extract_to = tempfile.mkdtemp()\n",
        "    print(f\"Extracting {zip_path} to {extract_to}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "    extracted_items = os.listdir(extract_to)\n",
        "    if len(extracted_items) == 1 and os.path.isdir(os.path.join(extract_to, extracted_items[0])):\n",
        "        extract_to = os.path.join(extract_to, extracted_items[0])\n",
        "    print(f\"Extraction complete. Data root: {extract_to}\")\n",
        "    return extract_to\n",
        "\n",
        "def load_case(case_dir):\n",
        "    csv_p = os.path.join(case_dir, \"node_feature.csv\")\n",
        "    txt_p = os.path.join(case_dir, \"Label.txt\")\n",
        "\n",
        "    if not os.path.exists(csv_p):\n",
        "        possible_csv = list(Path(case_dir).rglob(\"node_feature.csv\"))\n",
        "        if possible_csv:\n",
        "            csv_p = str(possible_csv[0])\n",
        "            case_dir = os.path.dirname(csv_p)\n",
        "            txt_p = os.path.join(case_dir, \"Label.txt\")\n",
        "\n",
        "    if not os.path.exists(csv_p):\n",
        "        raise FileNotFoundError(f\"Missing {csv_p}\")\n",
        "    if not os.path.exists(txt_p):\n",
        "        raise FileNotFoundError(f\"Missing {txt_p}\")\n",
        "\n",
        "    node_df = pd.read_csv(csv_p)\n",
        "    valid_mask = node_df['id'] == 1\n",
        "    df_valid = node_df[valid_mask].reset_index(drop=True)\n",
        "\n",
        "    with open(txt_p, \"r\") as f:\n",
        "        lines = [line.strip() for line in f if line.strip() != \"\"]\n",
        "\n",
        "    labels = [np.fromstring(line, sep=' ') for line in lines]\n",
        "    labels = np.vstack(labels)\n",
        "\n",
        "    if df_valid.shape[0] != labels.shape[0]:\n",
        "        raise ValueError(f\"Mismatch: valid node rows {df_valid.shape[0]} vs label rows {labels.shape[0]}\")\n",
        "\n",
        "    feats = df_valid[['width','height','x','y','angle']].values.astype(float)\n",
        "    coords = df_valid[['x','y']].values.astype(float)\n",
        "\n",
        "    return node_df, feats, coords, labels, df_valid\n",
        "\n",
        "def build_edge_index(n_nodes):\n",
        "    if n_nodes == 0:\n",
        "        return torch.empty((2,0), dtype=torch.long)\n",
        "    rows = []\n",
        "    cols = []\n",
        "    for i in range(n_nodes):\n",
        "        prev_neighbor = (i - 1) % n_nodes\n",
        "        rows.append(i)\n",
        "        cols.append(prev_neighbor)\n",
        "        next_neighbor = (i + 1) % n_nodes\n",
        "        rows.append(i)\n",
        "        cols.append(next_neighbor)\n",
        "    edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
        "    return edge_index\n",
        "\n",
        "def find_case_directories(data_root):\n",
        "    case_dirs = []\n",
        "    for root, dirs, files in os.walk(data_root):\n",
        "        if 'node_feature.csv' in files and 'Label.txt' in files:\n",
        "            case_dirs.append(root)\n",
        "\n",
        "    if not case_dirs:\n",
        "        potential_cases = [d for d in os.listdir(data_root)\n",
        "                          if os.path.isdir(os.path.join(data_root, d))]\n",
        "        for case in potential_cases:\n",
        "            case_path = os.path.join(data_root, case)\n",
        "            if (os.path.exists(os.path.join(case_path, 'node_feature.csv')) and\n",
        "                os.path.exists(os.path.join(case_path, 'Label.txt'))):\n",
        "                case_dirs.append(case_path)\n",
        "\n",
        "    case_dirs = [os.path.relpath(case, data_root) for case in case_dirs]\n",
        "    return sorted(case_dirs)\n",
        "\n",
        "class PaperPCA:\n",
        "    \"\"\"PCA implementation matching the paper's approach\"\"\"\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.mean_ = None\n",
        "        self.components_ = None\n",
        "        self.explained_variance_ = None\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "        X_centered = X - self.mean_\n",
        "        cov_matrix = np.cov(X_centered, rowvar=False)\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "        idx = np.argsort(eigenvalues)[::-1]\n",
        "        self.components_ = eigenvectors[:, idx[:self.n_components]]\n",
        "        self.explained_variance_ = eigenvalues[idx[:self.n_components]]\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_centered = X - self.mean_\n",
        "        return X_centered @ self.components_\n",
        "\n",
        "    def inverse_transform(self, X_transformed):\n",
        "        return X_transformed @ self.components_.T + self.mean_\n",
        "\n",
        "# ============================================================================\n",
        "# LIPSCHITZ EMBEDDINGS (Section 3.2.2 of paper)\n",
        "# ============================================================================\n",
        "\n",
        "def compute_lipschitz_embeddings(edge_index, num_nodes):\n",
        "    \"\"\"\n",
        "    Compute Lipschitz positional embeddings as described in the paper.\n",
        "    Uses shortest path distances from anchor nodes.\n",
        "    Number of anchors n = O(log(N)^2) as per Bourgain's theorem.\n",
        "    \"\"\"\n",
        "    if num_nodes <= 1:\n",
        "        return torch.zeros((num_nodes, 1), dtype=torch.float32)\n",
        "\n",
        "    # Number of anchors based on Bourgain's theorem\n",
        "    n_anchors = max(1, int(np.log(max(num_nodes, 2)) ** 2))\n",
        "    n_anchors = min(n_anchors, num_nodes)\n",
        "\n",
        "    # Select anchor nodes randomly (use fixed seed for reproducibility)\n",
        "    np.random.seed(42)\n",
        "    anchor_indices = np.random.choice(num_nodes, size=n_anchors, replace=False)\n",
        "\n",
        "    # Build adjacency list for shortest path computation\n",
        "    adj_list = [[] for _ in range(num_nodes)]\n",
        "    edge_index_np = edge_index.numpy()\n",
        "    for i in range(edge_index_np.shape[1]):\n",
        "        src, dst = edge_index_np[0, i], edge_index_np[1, i]\n",
        "        if src != dst:  # Avoid self-loops\n",
        "            adj_list[src].append(dst)\n",
        "\n",
        "    # Compute shortest path distances from each anchor using BFS\n",
        "    embeddings = np.zeros((num_nodes, n_anchors))\n",
        "\n",
        "    for anchor_idx, anchor in enumerate(anchor_indices):\n",
        "        # BFS from anchor\n",
        "        distances = np.full(num_nodes, np.inf)\n",
        "        distances[anchor] = 0\n",
        "        queue = [anchor]\n",
        "        visited = set([anchor])\n",
        "\n",
        "        while queue:\n",
        "            node = queue.pop(0)\n",
        "            for neighbor in adj_list[node]:\n",
        "                if neighbor not in visited:\n",
        "                    visited.add(neighbor)\n",
        "                    distances[neighbor] = distances[node] + 1\n",
        "                    queue.append(neighbor)\n",
        "\n",
        "        # Handle unreachable nodes (set to max distance)\n",
        "        max_dist = num_nodes\n",
        "        distances[distances == np.inf] = max_dist\n",
        "        embeddings[:, anchor_idx] = distances\n",
        "\n",
        "    # Normalize embeddings to prevent numerical issues\n",
        "    mean = embeddings.mean(axis=0, keepdims=True)\n",
        "    std = embeddings.std(axis=0, keepdims=True) + 1e-8\n",
        "    embeddings = (embeddings - mean) / std\n",
        "\n",
        "    return torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "# ============================================================================\n",
        "# GRAPH LAPLACIAN AND EIGENVECTORS (Section 3.2.1 of paper)\n",
        "# ============================================================================\n",
        "\n",
        "def compute_normalized_laplacian_eigenvectors(edge_index, num_nodes, m):\n",
        "    \"\"\"\n",
        "    Compute first m eigenvectors of normalized graph Laplacian.\n",
        "    Uses LOBPCG algorithm as mentioned in the paper for efficiency O(mE).\n",
        "    Returns Q_m ∈ R^{N×m} containing first m eigenvectors.\n",
        "    \"\"\"\n",
        "    if num_nodes <= 2:\n",
        "        # For very small graphs, return identity\n",
        "        m_actual = min(m, num_nodes)\n",
        "        eigenvectors = torch.eye(num_nodes, m_actual, dtype=torch.float32)\n",
        "        eigenvalues = torch.zeros(m_actual, dtype=torch.float32)\n",
        "        return eigenvectors, eigenvalues\n",
        "\n",
        "    # Build adjacency matrix\n",
        "    edge_index_np = edge_index.numpy()\n",
        "    row = edge_index_np[0]\n",
        "    col = edge_index_np[1]\n",
        "    data = np.ones(len(row))\n",
        "\n",
        "    # Create sparse adjacency matrix\n",
        "    adj = sp.coo_matrix((data, (row, col)), shape=(num_nodes, num_nodes))\n",
        "    adj = adj.tocsr()\n",
        "\n",
        "    # Make adjacency symmetric (undirected graph)\n",
        "    adj = adj + adj.T\n",
        "    adj = (adj > 0).astype(float)\n",
        "\n",
        "    # Compute degree matrix\n",
        "    degree = np.array(adj.sum(axis=1)).flatten()\n",
        "    degree[degree == 0] = 1  # Avoid division by zero for isolated nodes\n",
        "\n",
        "    # D^{-1/2}\n",
        "    d_inv_sqrt = sp.diags(1.0 / np.sqrt(degree))\n",
        "\n",
        "    # Normalized Laplacian: L = I - D^{-1/2} A D^{-1/2}\n",
        "    identity = sp.eye(num_nodes)\n",
        "    laplacian = identity - d_inv_sqrt @ adj @ d_inv_sqrt\n",
        "\n",
        "    # Compute first m eigenvectors using eigsh\n",
        "    m_actual = min(m, num_nodes - 2)\n",
        "    m_actual = max(1, m_actual)  # At least 1 eigenvector\n",
        "\n",
        "    try:\n",
        "        # We want smallest eigenvalues (lowest frequencies)\n",
        "        eigenvalues, eigenvectors = eigsh(laplacian, k=m_actual, which='SM', maxiter=1000)\n",
        "\n",
        "        # Sort by eigenvalues (should already be sorted, but ensure)\n",
        "        idx = np.argsort(eigenvalues)\n",
        "        eigenvalues = eigenvalues[idx]\n",
        "        eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: eigsh failed with error: {e}\")\n",
        "        print(f\"Falling back to dense eigendecomposition for {num_nodes} nodes\")\n",
        "        laplacian_dense = laplacian.toarray()\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(laplacian_dense)\n",
        "        eigenvalues = eigenvalues[:m_actual]\n",
        "        eigenvectors = eigenvectors[:, :m_actual]\n",
        "\n",
        "    return torch.tensor(eigenvectors, dtype=torch.float32), torch.tensor(eigenvalues, dtype=torch.float32)\n",
        "\n",
        "# ============================================================================\n",
        "# SP2GNO ARCHITECTURE COMPONENTS (Section 3 of paper)\n",
        "# ============================================================================\n",
        "\n",
        "class SpectralConvLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Spectral graph convolution layer using truncated GFT (Section 3.2.1).\n",
        "    Implements: v_{j+1}^{spectral} = σ(Q_m · K ×_1 (Q_m^T · v_j(x)) + w(v_j(x)))\n",
        "\n",
        "    Uses 3D kernel K ∈ R^{m×d×d} for enriched parameter space.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim, m_eigenvectors):\n",
        "        super(SpectralConvLayer, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.m = m_eigenvectors\n",
        "\n",
        "        # 3D kernel for spectral filtering (m × in_dim × out_dim)\n",
        "        self.spectral_kernel = nn.Parameter(torch.randn(m_eigenvectors, in_dim, out_dim) * 0.01)\n",
        "\n",
        "        # Residual connection (w in paper)\n",
        "        self.residual = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "    def forward(self, x, Q_m):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Node features [N, in_dim]\n",
        "            Q_m: First m eigenvectors [N, m]\n",
        "        Returns:\n",
        "            out: Transformed features [N, out_dim]\n",
        "        \"\"\"\n",
        "        N, in_dim = x.shape\n",
        "        m = Q_m.shape[1]\n",
        "\n",
        "        # Ensure dimensions match\n",
        "        if Q_m.shape[0] != N:\n",
        "            raise ValueError(f\"Q_m shape mismatch: expected [{N}, {m}], got {Q_m.shape}\")\n",
        "\n",
        "        # Graph Fourier Transform: Q_m^T · x → [m, in_dim]\n",
        "        x_spectral = torch.matmul(Q_m.t(), x)\n",
        "\n",
        "        # Mode-1 tensor-matrix product: K ×_1 x_spectral\n",
        "        # Einstein notation: mpd,mp->md (contract first mode)\n",
        "        x_filtered = torch.einsum('mpd,mp->md', self.spectral_kernel, x_spectral)\n",
        "\n",
        "        # Inverse GFT: Q_m · x_filtered → [N, out_dim]\n",
        "        x_out = torch.matmul(Q_m, x_filtered)\n",
        "\n",
        "        # Add residual connection\n",
        "        x_out = x_out + self.residual(x)\n",
        "\n",
        "        # Apply activation\n",
        "        out = F.gelu(x_out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SpatialConvLayerWithGating(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial graph convolution with gating mechanism (Section 3.2.2).\n",
        "    Implements: v_{j+1}^{spatial}_u = Σ_{v∈N(u)} γ_{uv} W v_j(x)_v\n",
        "\n",
        "    Gating mechanism learns edge weights using Lipschitz embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim, edge_dim=16, max_lipschitz_dim=64):\n",
        "        super(SpatialConvLayerWithGating, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.max_lipschitz_dim = max_lipschitz_dim\n",
        "\n",
        "        # Feature transformation W\n",
        "        self.weight = nn.Linear(in_dim, out_dim)\n",
        "\n",
        "        # W_2: Edge weight encoding\n",
        "        self.edge_encoder = nn.Linear(1, edge_dim)\n",
        "\n",
        "        # Projection layer for variable Lipschitz dimensions\n",
        "        self.lipschitz_proj = nn.Linear(max_lipschitz_dim, edge_dim)\n",
        "\n",
        "        # W_1, W_3: Gating MLP (now uses projected embeddings)\n",
        "        gate_hidden = max(32, (2 * edge_dim + edge_dim) // 2)\n",
        "        self.gate_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * edge_dim + edge_dim, gate_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(gate_hidden, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, lipschitz_embed):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Node features [N, in_dim]\n",
        "            edge_index: Graph connectivity [2, E]\n",
        "            edge_weight: Initial edge weights [E] (Euclidean distances)\n",
        "            lipschitz_embed: Lipschitz embeddings [N, variable_dim]\n",
        "        Returns:\n",
        "            out: Aggregated features [N, out_dim]\n",
        "        \"\"\"\n",
        "        N = x.size(0)\n",
        "\n",
        "        # Ensure edge_index contains valid indices\n",
        "        if edge_index.numel() > 0:\n",
        "            max_idx = edge_index.max().item()\n",
        "            if max_idx >= N:\n",
        "                raise ValueError(f\"Edge index contains invalid node index {max_idx} >= {N}\")\n",
        "\n",
        "        src, dst = edge_index[0], edge_index[1]\n",
        "\n",
        "        # Ensure lipschitz embeddings match expected dimension\n",
        "        if lipschitz_embed.size(0) != N:\n",
        "            raise ValueError(f\"Lipschitz embedding size mismatch: expected {N}, got {lipschitz_embed.size(0)}\")\n",
        "\n",
        "        # Handle empty graph case\n",
        "        if edge_index.shape[1] == 0:\n",
        "            return torch.zeros(N, self.out_dim, device=x.device, dtype=x.dtype)\n",
        "\n",
        "        # Project Lipschitz embeddings to fixed dimension\n",
        "        current_dim = lipschitz_embed.size(1)\n",
        "        if current_dim < self.max_lipschitz_dim:\n",
        "            # Pad with zeros if dimension is smaller\n",
        "            padding = torch.zeros(N, self.max_lipschitz_dim - current_dim,\n",
        "                                device=lipschitz_embed.device, dtype=lipschitz_embed.dtype)\n",
        "            lipschitz_embed_padded = torch.cat([lipschitz_embed, padding], dim=1)\n",
        "        elif current_dim > self.max_lipschitz_dim:\n",
        "            # Truncate if dimension is larger\n",
        "            lipschitz_embed_padded = lipschitz_embed[:, :self.max_lipschitz_dim]\n",
        "        else:\n",
        "            lipschitz_embed_padded = lipschitz_embed\n",
        "\n",
        "        # Project to edge_dim\n",
        "        lipschitz_proj = self.lipschitz_proj(lipschitz_embed_padded)  # [N, edge_dim]\n",
        "\n",
        "        # Encode edge weights (W_2 * w_uv)\n",
        "        edge_features = self.edge_encoder(edge_weight.unsqueeze(-1))  # [E, edge_dim]\n",
        "\n",
        "        # Concatenate embeddings: [h_v || h_u || W_2 w_uv]\n",
        "        h_src = lipschitz_proj[src]  # [E, edge_dim]\n",
        "        h_dst = lipschitz_proj[dst]  # [E, edge_dim]\n",
        "        gate_input = torch.cat([h_dst, h_src, edge_features], dim=-1)  # [E, 3*edge_dim]\n",
        "\n",
        "        # Compute edge gates γ_uv (Eq. 27)\n",
        "        edge_gates = self.gate_mlp(gate_input).squeeze(-1)  # [E]\n",
        "\n",
        "        # Transform node features\n",
        "        x_transformed = self.weight(x)  # [N, out_dim]\n",
        "\n",
        "        # Message passing with gating: Γ ⊙ A v W\n",
        "        messages = x_transformed[src] * edge_gates.unsqueeze(-1)  # [E, out_dim]\n",
        "\n",
        "        # Aggregate messages\n",
        "        out = torch.zeros(N, self.out_dim, device=x.device, dtype=x.dtype)\n",
        "        out.index_add_(0, dst, messages)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Sp2GNOBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Single Sp2GNO block combining spatial and spectral convolutions (Section 3.2).\n",
        "    Implements parallel processing and collaboration mechanism (Eq. 29-31).\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, m_eigenvectors, max_lipschitz_dim=64, edge_dim=16):\n",
        "        super(Sp2GNOBlock, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Spectral convolution branch\n",
        "        self.spectral_conv = SpectralConvLayer(hidden_dim, hidden_dim, m_eigenvectors)\n",
        "\n",
        "        # Spatial convolution branch with gating\n",
        "        self.spatial_conv = SpatialConvLayerWithGating(hidden_dim, hidden_dim,\n",
        "                                                       edge_dim=edge_dim,\n",
        "                                                       max_lipschitz_dim=max_lipschitz_dim)\n",
        "\n",
        "        # Collaboration mechanism: f([v_spectral || v_spatial])\n",
        "        # Linear layer to combine both branches (Eq. 31)\n",
        "        self.collaboration = nn.Sequential(\n",
        "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, Q_m, lipschitz_embed):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Node features [N, hidden_dim]\n",
        "            edge_index: Graph connectivity [2, E]\n",
        "            edge_weight: Edge weights [E]\n",
        "            Q_m: First m eigenvectors [N, m]\n",
        "            lipschitz_embed: Lipschitz embeddings [N, lipschitz_dim]\n",
        "        Returns:\n",
        "            out: Updated node features [N, hidden_dim]\n",
        "        \"\"\"\n",
        "        # Spectral branch (Eq. 30)\n",
        "        x_spectral = self.spectral_conv(x, Q_m)\n",
        "\n",
        "        # Spatial branch (Eq. 29)\n",
        "        x_spatial = self.spatial_conv(x, edge_index, edge_weight, lipschitz_embed)\n",
        "\n",
        "        # Concatenate outputs (Eq. 31)\n",
        "        x_concat = torch.cat([x_spectral, x_spatial], dim=-1)\n",
        "\n",
        "        # Collaboration: learn optimal combination\n",
        "        x_out = self.collaboration(x_concat)\n",
        "\n",
        "        # Residual connection and normalization\n",
        "        x_out = self.norm(x_out + x)\n",
        "\n",
        "        return x_out\n",
        "\n",
        "\n",
        "class Sp2GNO(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Spatio-Spectral Graph Neural Operator (Section 3.1).\n",
        "    Architecture: P → S_1 → S_2 → ... → S_L → Q\n",
        "    where S_i are Sp2GNO blocks.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, hidden_dim=128, num_blocks=4,\n",
        "                 m_eigenvectors=32, max_lipschitz_dim=64, edge_dim=16):\n",
        "        super(Sp2GNO, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_blocks = num_blocks\n",
        "        self.m = m_eigenvectors\n",
        "        self.max_lipschitz_dim = max_lipschitz_dim\n",
        "\n",
        "        # Uplifting layer P: R^{d_init} → R^d\n",
        "        self.uplift = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Stack of Sp2GNO blocks S_1, ..., S_L\n",
        "        self.sp2gno_blocks = nn.ModuleList([\n",
        "            Sp2GNOBlock(hidden_dim, m_eigenvectors, max_lipschitz_dim, edge_dim)\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        # Downlifting layer Q: R^d → R^{d_u}\n",
        "        # Two heads: one for PCA coefficients, one for uncertainty\n",
        "        self.q_pca = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, out_features)\n",
        "        )\n",
        "\n",
        "        self.q_var = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data: PyG Data object with attributes:\n",
        "                - x: Node features [N, in_features]\n",
        "                - edge_index: Graph connectivity [2, E]\n",
        "                - edge_weight: Edge weights [E]\n",
        "                - Q_m: First m eigenvectors [N, m]\n",
        "                - lipschitz_embed: Lipschitz embeddings [N, lipschitz_dim]\n",
        "        Returns:\n",
        "            pca_coeffs: Predicted PCA coefficients [N, out_features]\n",
        "            variance: Predicted uncertainty [N, 1]\n",
        "        \"\"\"\n",
        "        # Handle batched data (PyG batches graphs by concatenating them)\n",
        "        # For single graphs, batch info is not present\n",
        "\n",
        "        # Uplift features: v_0 = P({a, x})\n",
        "        x = self.uplift(data.x)\n",
        "\n",
        "        # Process through Sp2GNO blocks: v_j+1 = S_j+1(v_j)\n",
        "        for block in self.sp2gno_blocks:\n",
        "            x = block(x, data.edge_index, data.edge_weight,\n",
        "                     data.Q_m, data.lipschitz_embed)\n",
        "\n",
        "        # Downlift to outputs: u = Q(v_L)\n",
        "        pca_coeffs = self.q_pca(x)\n",
        "        variance = self.q_var(x)\n",
        "        variance = F.softplus(variance) + 1e-6  # Ensure positivity\n",
        "\n",
        "        return pca_coeffs, variance\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class BoxGirderGraphDataset(Dataset):\n",
        "    def __init__(self, root_dir, case_dirs, pca=None, train_mode=True, m_eigenvectors=M_EIGENVECTORS):\n",
        "        super().__init__()\n",
        "        self.root_dir = root_dir\n",
        "        self.case_dirs = case_dirs\n",
        "        self.pca = pca\n",
        "        self.train_mode = train_mode\n",
        "        self.m = m_eigenvectors\n",
        "        self.data_list = []\n",
        "        self._prepare()\n",
        "\n",
        "    def _prepare(self):\n",
        "        for case in self.case_dirs:\n",
        "            cdir = os.path.join(self.root_dir, case)\n",
        "            try:\n",
        "                node_df, feats, coords, labels, df_valid = load_case(cdir)\n",
        "                n_nodes = feats.shape[0]\n",
        "\n",
        "                # Build edge index using original circular method (more suitable for box girder)\n",
        "                edge_index = build_edge_index(n_nodes)\n",
        "\n",
        "                # Compute edge weights (Euclidean distances, Eq. 23)\n",
        "                edge_weight = []\n",
        "                for i in range(edge_index.shape[1]):\n",
        "                    src, dst = edge_index[0, i].item(), edge_index[1, i].item()\n",
        "                    dist = np.linalg.norm(coords[src] - coords[dst]) + 1e-6\n",
        "                    edge_weight.append(dist)\n",
        "                edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n",
        "\n",
        "                # Compute first m eigenvectors of normalized Laplacian (Section 3.2.1)\n",
        "                Q_m, eigenvalues = compute_normalized_laplacian_eigenvectors(\n",
        "                    edge_index, n_nodes, self.m\n",
        "                )\n",
        "\n",
        "                # Compute Lipschitz embeddings (Section 3.2.2)\n",
        "                lipschitz_embed = compute_lipschitz_embeddings(edge_index, n_nodes)\n",
        "\n",
        "                # Transform labels to PCA coefficients\n",
        "                if self.pca is not None:\n",
        "                    phi = self.pca.transform(labels)\n",
        "                else:\n",
        "                    phi = None\n",
        "\n",
        "                # Create PyG Data object\n",
        "                x = torch.tensor(feats, dtype=torch.float)\n",
        "                if phi is not None:\n",
        "                    y = torch.tensor(phi, dtype=torch.float)\n",
        "                else:\n",
        "                    y = None\n",
        "\n",
        "                data = Data(\n",
        "                    x=x,\n",
        "                    edge_index=edge_index,\n",
        "                    edge_weight=edge_weight,\n",
        "                    y=y,\n",
        "                    Q_m=Q_m,\n",
        "                    eigenvalues=eigenvalues,\n",
        "                    lipschitz_embed=lipschitz_embed,\n",
        "                    orig_labels=torch.tensor(labels, dtype=torch.float),\n",
        "                    coords=torch.tensor(coords, dtype=torch.float),\n",
        "                    case_name=case\n",
        "                )\n",
        "\n",
        "                # Validate dimensions\n",
        "                assert x.shape[0] == Q_m.shape[0], f\"Node count mismatch: x={x.shape[0]}, Q_m={Q_m.shape[0]}\"\n",
        "                assert x.shape[0] == lipschitz_embed.shape[0], f\"Node count mismatch: x={x.shape[0]}, lipschitz={lipschitz_embed.shape[0]}\"\n",
        "                assert edge_index.shape[1] == edge_weight.shape[0], f\"Edge count mismatch: edge_index={edge_index.shape[1]}, edge_weight={edge_weight.shape[0]}\"\n",
        "                if y is not None:\n",
        "                    assert x.shape[0] == y.shape[0], f\"Node count mismatch: x={x.shape[0]}, y={y.shape[0]}\"\n",
        "\n",
        "                self.data_list.append(data)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not load case {case}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self.data_list[idx]\n",
        "\n",
        "# ============================================================================\n",
        "# LOSS FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "class PCALoss(nn.Module):\n",
        "    \"\"\"Loss for PCA coefficient prediction\"\"\"\n",
        "    def __init__(self, m_components, use_weights=True):\n",
        "        super().__init__()\n",
        "        self.m_components = m_components\n",
        "        self.use_weights = use_weights\n",
        "\n",
        "    def forward(self, pred_phi, target_phi, variance=None, pca_weights=None):\n",
        "        # MSE loss for PCA coefficients\n",
        "        squared_errors = (pred_phi - target_phi) ** 2\n",
        "\n",
        "        # Weight by PCA explained variance\n",
        "        if self.use_weights and pca_weights is not None:\n",
        "            weights = pca_weights.to(pred_phi.device)\n",
        "            weighted_errors = squared_errors * weights\n",
        "        else:\n",
        "            weighted_errors = squared_errors\n",
        "\n",
        "        loss_pca = weighted_errors.mean()\n",
        "\n",
        "        # Variance loss (negative log-likelihood)\n",
        "        if variance is not None:\n",
        "            nll = 0.5 * (squared_errors / variance + torch.log(variance))\n",
        "            loss_var = nll.mean()\n",
        "            return loss_pca + 0.1 * loss_var\n",
        "\n",
        "        return loss_pca\n",
        "\n",
        "class SpatialSmoothnessLoss(nn.Module):\n",
        "    \"\"\"Regularization for spatial smoothness\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pca_pred, edge_index):\n",
        "        src, dst = edge_index[0], edge_index[1]\n",
        "        diff = pca_pred[src] - pca_pred[dst]\n",
        "        smoothness = (diff ** 2).mean()\n",
        "        return smoothness\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING AND EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_fn, smooth_loss_fn, device, pca_weights=None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for data in loader:\n",
        "        try:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pca_pred, var_pred = model(data)\n",
        "            target = data.y\n",
        "\n",
        "            # Main loss\n",
        "            loss = loss_fn(pca_pred, target, var_pred, pca_weights)\n",
        "\n",
        "            # Spatial smoothness regularization\n",
        "            smooth_loss = smooth_loss_fn(pca_pred, data.edge_index)\n",
        "\n",
        "            # Total loss\n",
        "            total_loss_batch = loss + 0.01 * smooth_loss\n",
        "            total_loss_batch.backward()\n",
        "\n",
        "            # Gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += total_loss_batch.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in training batch: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    return total_loss / max(num_batches, 1)\n",
        "\n",
        "def evaluate(model, dataset, pca, device):\n",
        "    model.eval()\n",
        "    results = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in dataset.data_list:\n",
        "            try:\n",
        "                data = data.to(device)\n",
        "                pca_pred, var_pred = model(data)\n",
        "\n",
        "                pca_pred = pca_pred.cpu().numpy()\n",
        "                var_pred = var_pred.cpu().numpy()\n",
        "\n",
        "                # Reconstruct time series\n",
        "                recon = pca.inverse_transform(pca_pred)\n",
        "                true_ts = data.orig_labels.cpu().numpy()\n",
        "\n",
        "                n_nodes = true_ts.shape[0]\n",
        "                rmse_list = []\n",
        "                r2_list = []\n",
        "\n",
        "                for i in range(n_nodes):\n",
        "                    y_true = true_ts[i]\n",
        "                    y_pred = recon[i]\n",
        "                    rmse_list.append(math.sqrt(mean_squared_error(y_true, y_pred)))\n",
        "                    r2_list.append(r2_score(y_true, y_pred))\n",
        "\n",
        "                # Peak metrics\n",
        "                true_peaks = true_ts.max(axis=1)\n",
        "                pred_peaks = recon.max(axis=1)\n",
        "                peak_mape = np.mean(np.abs((true_peaks - pred_peaks) / (true_peaks + 1e-8)))\n",
        "\n",
        "                # Impulse metrics\n",
        "                true_impulses = np.array([np.trapz(ts) for ts in true_ts])\n",
        "                pred_impulses = np.array([np.trapz(ts) for ts in recon])\n",
        "                impulse_mape = np.mean(np.abs((true_impulses - pred_impulses) / (true_impulses + 1e-8)))\n",
        "\n",
        "                overall_r2 = np.mean(r2_list)\n",
        "\n",
        "                results[data.case_name] = {\n",
        "                    'rmse_mean': float(np.mean(rmse_list)),\n",
        "                    'r2_mean': float(overall_r2),\n",
        "                    'peak_mape': float(peak_mape),\n",
        "                    'impulse_mape': float(impulse_mape),\n",
        "                    'variance_mean': float(np.mean(var_pred))\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error evaluating case {data.case_name}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "    return results\n",
        "\n",
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_results(model, test_dataset, pca, device, results):\n",
        "    \"\"\"Generate comprehensive visualizations\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if len(test_dataset) == 0:\n",
        "        print(\"No test cases to visualize\")\n",
        "        return\n",
        "\n",
        "    sample_data = test_dataset.get(0)\n",
        "    case_name = sample_data.case_name\n",
        "\n",
        "    print(f\"Visualizing case: {case_name}\")\n",
        "\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            sample_data = sample_data.to(device)\n",
        "            pca_pred, var_pred = model(sample_data)\n",
        "\n",
        "            pca_pred = pca_pred.cpu().numpy()\n",
        "            var_pred = var_pred.cpu().numpy()\n",
        "\n",
        "            # Reconstruct time series\n",
        "            recon = pca.inverse_transform(pca_pred)\n",
        "            true_ts = sample_data.orig_labels.cpu().numpy()\n",
        "            coords = sample_data.coords.cpu().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during visualization: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # 1. Time series comparison\n",
        "    ax1 = plt.subplot(2, 3, 1)\n",
        "    r2_scores = [r2_score(true_ts[i], recon[i]) for i in range(len(true_ts))]\n",
        "    best_idx = np.argmax(r2_scores)\n",
        "    worst_idx = np.argmin(r2_scores)\n",
        "\n",
        "    time_points = np.arange(len(true_ts[0]))\n",
        "    ax1.plot(time_points, true_ts[best_idx], 'b-', label=f'True (Best R²={r2_scores[best_idx]:.3f})', linewidth=2)\n",
        "    ax1.plot(time_points, recon[best_idx], 'r--', label='Predicted (Best)', linewidth=2)\n",
        "    ax1.plot(time_points, true_ts[worst_idx], 'g-', label=f'True (Worst R²={r2_scores[worst_idx]:.3f})', linewidth=1, alpha=0.7)\n",
        "    ax1.plot(time_points, recon[worst_idx], 'm--', label='Predicted (Worst)', linewidth=1, alpha=0.7)\n",
        "    ax1.set_xlabel('Time Step')\n",
        "    ax1.set_ylabel('Overpressure')\n",
        "    ax1.set_title(f'Time Series Comparison\\n{case_name}')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Spatial distribution of R² scores\n",
        "    ax2 = plt.subplot(2, 3, 2)\n",
        "    scatter = ax2.scatter(coords[:, 0], coords[:, 1], c=r2_scores,\n",
        "                         cmap='RdYlGn', s=100, vmin=0, vmax=1, edgecolors='black')\n",
        "    ax2.set_xlabel('X Coordinate')\n",
        "    ax2.set_ylabel('Y Coordinate')\n",
        "    ax2.set_title('Spatial Distribution of R² Scores')\n",
        "    plt.colorbar(scatter, ax=ax2, label='R² Score')\n",
        "\n",
        "    # 3. Spatial distribution of predicted variance\n",
        "    ax3 = plt.subplot(2, 3, 3)\n",
        "    scatter = ax3.scatter(coords[:, 0], coords[:, 1], c=var_pred.flatten(),\n",
        "                         cmap='viridis', s=100, edgecolors='black')\n",
        "    ax3.set_xlabel('X Coordinate')\n",
        "    ax3.set_ylabel('Y Coordinate')\n",
        "    ax3.set_title('Predicted Uncertainty (Variance)')\n",
        "    plt.colorbar(scatter, ax=ax3, label='Variance')\n",
        "\n",
        "    # 4. PCA coefficients comparison\n",
        "    ax4 = plt.subplot(2, 3, 4)\n",
        "    avg_true_pca = pca.transform(true_ts).mean(axis=0)\n",
        "    avg_pred_pca = pca_pred.mean(axis=0)\n",
        "    x_pos = np.arange(len(avg_true_pca))\n",
        "    width = 0.35\n",
        "    ax4.bar(x_pos - width/2, avg_true_pca, width, label='True', alpha=0.7)\n",
        "    ax4.bar(x_pos + width/2, avg_pred_pca, width, label='Predicted', alpha=0.7)\n",
        "    ax4.set_xlabel('PCA Component')\n",
        "    ax4.set_ylabel('Average Coefficient Value')\n",
        "    ax4.set_title('PCA Coefficients Comparison')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Peak overpressure comparison\n",
        "    ax5 = plt.subplot(2, 3, 5)\n",
        "    true_peaks = true_ts.max(axis=1)\n",
        "    pred_peaks = recon.max(axis=1)\n",
        "    ax5.scatter(true_peaks, pred_peaks, alpha=0.6, s=50)\n",
        "    max_val = max(max(true_peaks), max(pred_peaks))\n",
        "    ax5.plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "    ax5.set_xlabel('True Peak Overpressure')\n",
        "    ax5.set_ylabel('Predicted Peak Overpressure')\n",
        "    ax5.set_title('Peak Overpressure: True vs Predicted')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "\n",
        "    # 6. R² distribution\n",
        "    ax6 = plt.subplot(2, 3, 6)\n",
        "    ax6.hist(r2_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    ax6.axvline(np.mean(r2_scores), color='red', linestyle='--', linewidth=2,\n",
        "               label=f'Mean: {np.mean(r2_scores):.3f}')\n",
        "    ax6.set_xlabel('R² Score')\n",
        "    ax6.set_ylabel('Frequency')\n",
        "    ax6.set_title('Distribution of R² Scores Across Nodes')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('sp2gno_results.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Visualization saved to: sp2gno_results.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Additional plot: Overall performance across all test cases\n",
        "    if len(results) > 1:\n",
        "        fig2, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        case_names = list(results.keys())\n",
        "        r2_values = [results[c]['r2_mean'] for c in case_names]\n",
        "        rmse_values = [results[c]['rmse_mean'] for c in case_names]\n",
        "        peak_mape_values = [results[c]['peak_mape'] for c in case_names]\n",
        "        impulse_mape_values = [results[c]['impulse_mape'] for c in case_names]\n",
        "\n",
        "        axes[0, 0].bar(range(len(case_names)), r2_values, color='skyblue', alpha=0.7)\n",
        "        axes[0, 0].set_xlabel('Test Case')\n",
        "        axes[0, 0].set_ylabel('R² Score')\n",
        "        axes[0, 0].set_title('R² Score by Test Case')\n",
        "        axes[0, 0].set_xticks(range(len(case_names)))\n",
        "        axes[0, 0].set_xticklabels(case_names, rotation=45, ha='right')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[0, 1].bar(range(len(case_names)), rmse_values, color='lightcoral', alpha=0.7)\n",
        "        axes[0, 1].set_xlabel('Test Case')\n",
        "        axes[0, 1].set_ylabel('RMSE')\n",
        "        axes[0, 1].set_title('RMSE by Test Case')\n",
        "        axes[0, 1].set_xticks(range(len(case_names)))\n",
        "        axes[0, 1].set_xticklabels(case_names, rotation=45, ha='right')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[1, 0].bar(range(len(case_names)), peak_mape_values, color='lightgreen', alpha=0.7)\n",
        "        axes[1, 0].set_xlabel('Test Case')\n",
        "        axes[1, 0].set_ylabel('Peak MAPE')\n",
        "        axes[1, 0].set_title('Peak MAPE by Test Case')\n",
        "        axes[1, 0].set_xticks(range(len(case_names)))\n",
        "        axes[1, 0].set_xticklabels(case_names, rotation=45, ha='right')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        axes[1, 1].bar(range(len(case_names)), impulse_mape_values, color='gold', alpha=0.7)\n",
        "        axes[1, 1].set_xlabel('Test Case')\n",
        "        axes[1, 1].set_ylabel('Impulse MAPE')\n",
        "        axes[1, 1].set_title('Impulse MAPE by Test Case')\n",
        "        axes[1, 1].set_xticks(range(len(case_names)))\n",
        "        axes[1, 1].set_xticklabels(case_names, rotation=45, ha='right')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('sp2gno_overall_performance.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"Overall performance plot saved to: sp2gno_overall_performance.png\")\n",
        "        plt.close()\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\"*70)\n",
        "    print(\"Sp2GNO for PCA-compressed Time Series Prediction\")\n",
        "    print(\"Exact Architecture from Paper - FIXED VERSION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Random seed: {SEED}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Extract data\n",
        "    if not os.path.exists(ZIP_FILE_PATH):\n",
        "        print(f\"Error: Zip file not found at {ZIP_FILE_PATH}\")\n",
        "        print(\"Please update ZIP_FILE_PATH variable with the correct path.\")\n",
        "        return\n",
        "\n",
        "    data_root = extract_zip(ZIP_FILE_PATH)\n",
        "    case_dirs = find_case_directories(data_root)\n",
        "    print(f\"\\nFound {len(case_dirs)} cases\")\n",
        "\n",
        "    if not case_dirs:\n",
        "        print(\"No valid case directories found!\")\n",
        "        return\n",
        "\n",
        "    # Split train/test\n",
        "    if len(case_dirs) >= 12:\n",
        "        random.shuffle(case_dirs)\n",
        "        train_cases = case_dirs[:8]\n",
        "        test_cases = case_dirs[8:]\n",
        "    else:\n",
        "        n_train = max(1, int(0.66 * len(case_dirs)))\n",
        "        random.shuffle(case_dirs)\n",
        "        train_cases = case_dirs[:n_train]\n",
        "        test_cases = case_dirs[n_train:]\n",
        "\n",
        "    print(f\"Training cases: {len(train_cases)}\")\n",
        "    print(f\"Test cases: {len(test_cases)}\")\n",
        "\n",
        "    # Fit PCA on training data\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 1: Fitting PCA on training data\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    all_train_labels = []\n",
        "    successful_train_cases = []\n",
        "\n",
        "    for case in train_cases:\n",
        "        try:\n",
        "            _, _, _, labels, _ = load_case(os.path.join(data_root, case))\n",
        "            all_train_labels.append(labels)\n",
        "            successful_train_cases.append(case)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load training case {case}: {e}\")\n",
        "            continue\n",
        "\n",
        "    if not all_train_labels:\n",
        "        print(\"No training data could be loaded!\")\n",
        "        return\n",
        "\n",
        "    train_cases = successful_train_cases\n",
        "    all_train_labels = np.vstack(all_train_labels)\n",
        "    print(f\"Training label matrix shape: {all_train_labels.shape}\")\n",
        "\n",
        "    # Fit PCA\n",
        "    pca = PaperPCA(n_components=PCA_COMPONENTS)\n",
        "    pca.fit(all_train_labels)\n",
        "\n",
        "    total_variance = np.var(all_train_labels, axis=0).sum()\n",
        "    explained_variance_ratio = pca.explained_variance_ / total_variance\n",
        "    print(f\"Explained variance (first 5): {explained_variance_ratio[:5].round(4)}\")\n",
        "    print(f\"Cumulative explained variance: {np.sum(explained_variance_ratio):.4f}\")\n",
        "\n",
        "    # Create PCA weights for loss function\n",
        "    pca_weights = torch.tensor(explained_variance_ratio, dtype=torch.float32)\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 2: Creating graph datasets with Sp2GNO preprocessing\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    train_dataset = BoxGirderGraphDataset(\n",
        "        data_root, train_cases, pca=pca, train_mode=True, m_eigenvectors=M_EIGENVECTORS\n",
        "    )\n",
        "    test_dataset = BoxGirderGraphDataset(\n",
        "        data_root, test_cases, pca=pca, train_mode=False, m_eigenvectors=M_EIGENVECTORS\n",
        "    )\n",
        "\n",
        "    if len(train_dataset) == 0:\n",
        "        print(\"No training data available!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "    # Create data loaders - Use batch_size=1 because each graph has unique structure\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Get sample to determine dimensions\n",
        "    sample = train_dataset.get(0)\n",
        "    in_features = sample.x.shape[1]\n",
        "    num_nodes = sample.x.shape[0]\n",
        "\n",
        "    # Determine max Lipschitz dimension across all graphs\n",
        "    max_lipschitz_dim = 0\n",
        "    for data in train_dataset.data_list:\n",
        "        max_lipschitz_dim = max(max_lipschitz_dim, data.lipschitz_embed.shape[1])\n",
        "    for data in test_dataset.data_list:\n",
        "        max_lipschitz_dim = max(max_lipschitz_dim, data.lipschitz_embed.shape[1])\n",
        "\n",
        "    edge_dim = 16  # Fixed edge feature dimension\n",
        "\n",
        "    print(f\"\\nInput features: {in_features}\")\n",
        "    print(f\"Number of nodes (sample): {num_nodes}\")\n",
        "    print(f\"Output PCA components: {PCA_COMPONENTS}\")\n",
        "    print(f\"Max Lipschitz embedding dimension: {max_lipschitz_dim}\")\n",
        "    print(f\"Edge feature dimension: {edge_dim}\")\n",
        "    print(f\"Number of eigenvectors (m): {M_EIGENVECTORS}\")\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 3: Initializing Sp2GNO model (Paper Architecture)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    model = Sp2GNO(\n",
        "        in_features=in_features,\n",
        "        out_features=PCA_COMPONENTS,\n",
        "        hidden_dim=WIDTH,\n",
        "        num_blocks=NUM_SP2GNO_BLOCKS,\n",
        "        m_eigenvectors=M_EIGENVECTORS,\n",
        "        max_lipschitz_dim=max_lipschitz_dim,\n",
        "        edge_dim=edge_dim\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Number of Sp2GNO blocks: {NUM_SP2GNO_BLOCKS}\")\n",
        "    print(f\"Hidden dimension: {WIDTH}\")\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "\n",
        "    # Initialize optimizer and loss functions\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=20\n",
        "    )\n",
        "\n",
        "    loss_fn = PCALoss(m_components=PCA_COMPONENTS, use_weights=True)\n",
        "    smooth_loss_fn = SpatialSmoothnessLoss()\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 4: Training Sp2GNO\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience = 50\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train_loss = train_epoch(\n",
        "            model, train_loader, optimizer, loss_fn,\n",
        "            smooth_loss_fn, DEVICE, pca_weights\n",
        "        )\n",
        "\n",
        "        scheduler.step(train_loss)\n",
        "\n",
        "        if train_loss < best_loss:\n",
        "            best_loss = train_loss\n",
        "            patience_counter = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': best_loss,\n",
        "            }, \"best_sp2gno_model.pt\")\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"✓ New best model saved at epoch {epoch} with loss {best_loss:.6f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if epoch % 50 == 0 or epoch == 1:\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Epoch {epoch}/{EPOCHS} - Loss: {train_loss:.6f} - Best: {best_loss:.6f} - LR: {current_lr:.2e}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience and epoch > 200:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(\"best_sp2gno_model.pt\")\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"\\nLoaded best model from epoch {checkpoint['epoch']}\")\n",
        "\n",
        "    # Evaluation\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 5: Evaluation on test set\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    results = evaluate(model, test_dataset, pca, DEVICE)\n",
        "\n",
        "    print(\"\\nTest Results:\")\n",
        "    print(\"-\" * 70)\n",
        "    for case_name, metrics in results.items():\n",
        "        print(f\"{case_name}:\")\n",
        "        print(f\"  R²: {metrics['r2_mean']:.4f}\")\n",
        "        print(f\"  RMSE: {metrics['rmse_mean']:.4f}\")\n",
        "        print(f\"  Peak MAPE: {metrics['peak_mape']:.4f}\")\n",
        "        print(f\"  Impulse MAPE: {metrics['impulse_mape']:.4f}\")\n",
        "        print(f\"  Avg Variance: {metrics['variance_mean']:.4f}\")\n",
        "        print()\n",
        "\n",
        "    # Overall statistics\n",
        "    if results:\n",
        "        avg_r2 = np.mean([m['r2_mean'] for m in results.values()])\n",
        "        avg_rmse = np.mean([m['rmse_mean'] for m in results.values()])\n",
        "        avg_peak_mape = np.mean([m['peak_mape'] for m in results.values()])\n",
        "        avg_impulse_mape = np.mean([m['impulse_mape'] for m in results.values()])\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"OVERALL TEST PERFORMANCE\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Average R²: {avg_r2:.4f}\")\n",
        "        print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
        "        print(f\"Average Peak MAPE: {avg_peak_mape:.4f}\")\n",
        "        print(f\"Average Impulse MAPE: {avg_impulse_mape:.4f}\")\n",
        "\n",
        "    # Save model and PCA\n",
        "    torch.save(model.state_dict(), \"sp2gno_model.pt\")\n",
        "    joblib.dump(pca, \"sp2gno_pca.pkl\")\n",
        "    print(\"\\nModel saved to: sp2gno_model.pt\")\n",
        "    print(\"PCA saved to: sp2gno_pca.pkl\")\n",
        "\n",
        "    # Visualization\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"STEP 6: Generating visualizations\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    visualize_results(model, test_dataset, pca, DEVICE, results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uUmaN-cEEaU1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}